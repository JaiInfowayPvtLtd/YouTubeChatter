# Technical Reference: Chat with YouTube Videos

This document provides detailed technical information about the implementation of the Chat with YouTube Videos application. It's intended for developers who want to understand the codebase or make modifications.

## Architecture Overview

The application follows a simple architecture:

```
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│  Streamlit  │     │  RAG Utils  │     │  OpenAI API │
│  Interface  │────►│   Module    │────►│             │
└─────────────┘     └─────────────┘     └─────────────┘
       ▲                   ▲                   ▲
       │                   │                   │
       ▼                   ▼                   ▼
┌─────────────┐     ┌─────────────┐     ┌─────────────┐
│   YouTube   │     │  Transcript │     │    Model    │
│    Video    │────►│  Processing │────►│  Responses  │
└─────────────┘     └─────────────┘     └─────────────┘
```

## Code Structure

### 1. Main Application (`app.py`)

This file contains the Streamlit interface and main application logic:

- **Page Configuration**: Sets up the Streamlit page with title, icon, and layout
- **Session State**: Manages persistent state across Streamlit reruns
- **Video Processing**: Extracts and processes YouTube video transcripts
- **Chat Interface**: Handles user questions and displays AI responses

Key functions:
- `extract_video_id(url)`: Parses YouTube URLs to extract the video ID
- `get_transcript(video_id)`: Retrieves the transcript from a YouTube video
- `process_video()`: Main function for processing video URLs
- `handle_chat_input()`: Handles user questions and gets responses

### 2. RAG Utilities (`rag_utils.py`)

This file contains the `YoutubeVideoRAG` class that implements the RAG functionality:

```python
class YoutubeVideoRAG:
    def __init__(self, transcript, api_key=None):
        # Initialize with transcript and API key
        
    def query(self, question):
        # Process questions with OpenAI
        
    def summarize(self):
        # Generate summary of video content
```

- **Initialization**: Sets up the OpenAI client and processes the transcript
- **Query Processing**: Sends the transcript and question to the OpenAI API
- **Summary Generation**: Creates an overall summary of the video content

## Workflow Sequence

1. **User inputs YouTube URL**:
   - URL is validated and video ID is extracted
   - Transcript is retrieved using YouTube Transcript API

2. **Transcript processing**:
   - Transcript is cleaned and formatted
   - RAG engine is initialized with the transcript

3. **Question answering**:
   - User asks a question in the chat interface
   - Question is sent to the RAG engine along with the transcript
   - Response is generated by OpenAI's GPT-4o model
   - Answer is displayed in the chat interface

## Technical Details

### YouTube Transcript API Integration

The application uses `youtube_transcript_api` to fetch video transcripts:

```python
from youtube_transcript_api import YouTubeTranscriptApi, TranscriptsDisabled, NoTranscriptFound

def get_transcript(video_id):
    try:
        transcript_list = YouTubeTranscriptApi.get_transcript(video_id)
        transcript_text = " ".join([t["text"] for t in transcript_list])
        return transcript_text
    except (TranscriptsDisabled, NoTranscriptFound):
        return None
    except Exception as e:
        st.error(f"Error fetching transcript: {str(e)}")
        return None
```

Key aspects:
- Handles common errors like disabled or missing transcripts
- Concatenates transcript segments into a single text
- Returns `None` if transcript cannot be retrieved

### OpenAI Integration

The application interfaces with OpenAI through the official Python client:

```python
from openai import OpenAI

# Initialize client
client = OpenAI(api_key=api_key)

# Generate response
response = client.chat.completions.create(
    model="gpt-4o",
    messages=[
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt}
    ],
    max_tokens=1000
)
```

Key aspects:
- Uses the newest model (gpt-4o) for optimal performance
- Implements a system prompt to guide the AI's responses
- Combines the transcript and user question in the user prompt
- Handles token limits for large transcripts

### Streamlit User Interface

The interface is built with Streamlit and includes:

- **Sidebar**: For video input and processing
- **Main Area**: For chat interaction
- **Session State**: For maintaining chat history and application state

```python
# Example of Streamlit session state usage
if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

# Example of displaying chat messages
for message in st.session_state.chat_history:
    if message["role"] == "user":
        st.markdown(f"**You:** {message['content']}")
    else:
        st.markdown(f"**Assistant:** {message['content']}")
```

## Error Handling

The application implements comprehensive error handling:

1. **YouTube URL Validation**: Checks if the provided URL is a valid YouTube URL
2. **Transcript Retrieval Errors**: Handles cases where transcripts are not available
3. **API Errors**: Catches and displays errors from the OpenAI API
4. **Token Limit Management**: Truncates long transcripts to fit within OpenAI's context window

## Performance Considerations

- **Transcript Size**: Large transcripts are truncated to 16,000 tokens
- **Response Generation**: Responses are limited to 1,000 tokens for efficiency
- **Error Recovery**: Application maintains state even when errors occur

## Customization Options

### Modifying the System Prompt

The system prompt guides the AI's behavior. You can modify it in `rag_utils.py`:

```python
system_prompt = """You are an AI assistant specialized in analyzing YouTube video content.
Answer the question based on the transcript provided.
Be detailed and specific, referring to the actual content from the video.
If the answer is not in the transcript, politely state that you don't have that information from the video.
Format your responses in a clear and readable way."""
```

### Changing the OpenAI Model

You can change the model used by modifying the `model` parameter:

```python
response = client.chat.completions.create(
    model="gpt-4o",  # Change to a different model if needed
    # ...
)
```

### Adjusting Token Limits

You can adjust the token limits for transcript processing and response generation:

```python
# Maximum tokens for transcript
max_tokens = 16000  # Adjust as needed

# Maximum tokens for response
response = client.chat.completions.create(
    # ...
    max_tokens=1000  # Adjust as needed
)
```